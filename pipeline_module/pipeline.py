import spacy

from pipeline_module.declination import TextDeclinationObj
from pipeline_module.interfaces import InputPipelineData, OutputPipelineData, PatternKeyPhrases, PatternConfig, NerConfig, \
    FoundPhrases
from pipeline_module.keybert_wrapper import CustomKeyBertForArchive
from pipeline_module.ner import filter_ner
from pipeline_module.ocr import RussianPDFOCR
from pipeline_module.phrase_extractor import PhraseCountVectorizerWrapper


class TextProcessingPipeline:
    def __init__(self, spacy_model_name: str, bert_model_name: str):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.spacy_nlp_model = spacy.load(spacy_model_name)
        self.spacy_nlp_model.max_length = 400000  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞
        self.bert_extractor = CustomKeyBertForArchive(bert_model_name)
        self.phrase_extractor = PhraseCountVectorizerWrapper(self.spacy_nlp_model)
        self.ocr = RussianPDFOCR()

    @staticmethod
    def get_default_config() -> InputPipelineData:
        patterns = [
            PatternConfig("–û–¥–∏–Ω–æ—á–Ω—ã–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ", "one_noun", "<N.*>", 0.3, 10, 0.1),
            PatternConfig("–ë–∏–≥—Ä–∞–º–º—ã —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö", "bigramm_noun", "<N.*><ADP>?<N.*>", 0.4, 10, 0.3),
            PatternConfig("–ë–∏–≥—Ä–∞–º–º—ã –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö", "bigramm_adj_noun", "<ADJ><ADP>?<N.*>", 0.6, 10,
                          0.3),
            PatternConfig("–ë–æ–ª—å—à–∏–µ —Ñ—Ä–∞–∑—ã", "long_phrases", "(<A.*>?<ADP>?<A.*>?<N.*>){3,}", 0.7, 15, 0.4),
        ]
        return InputPipelineData(
            phrases_config=patterns,
            ner_config=NerConfig(
                input_threshold=15,
                exclude_types=["PER"],
                phrase_amount=80
            )
        )

    def process_text(self, document_to_process, config: InputPipelineData = None) -> OutputPipelineData:
        if config is None:
            config = self.get_default_config()

        print(f'–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞')
        text_from_document = self.ocr.recognize_pdf(document_to_process)
        processed_through_nlp_text = self.spacy_nlp_model(text_from_document)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ—Ä–∞–∑ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        print(f'–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ—Ä–∞–∑ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º')
        found_phrases = self.phrase_extractor.get_key_phrases(text_from_document, config.phrases_config)
        print("found_phrases", found_phrases, sep=" ")

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ —Å –ø–æ–º–æ—â—å—é BERT
        print(f'–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑ —Å –ø–æ–º–æ—â—å—é BERT')
        key_phrases = self.bert_extractor.extract_keywords(text_from_document, found_phrases)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ NER-—Å—É—â–Ω–æ—Å—Ç–µ–π
        print(f'–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ NER-—Å—É—â–Ω–æ—Å—Ç–µ–π')
        total_ner_list = filter_ner(processed_through_nlp_text.ents, config.ner_config.input_threshold, config.ner_config.exclude_types, config.ner_config.phrase_amount)

        # –°–∫–ª–æ–Ω–µ–Ω–∏–µ —Ñ—Ä–∞–∑
        print(f'–°–∫–ª–æ–Ω–µ–Ω–∏–µ NER —Ñ—Ä–∞–∑')
        declination_obj = TextDeclinationObj(processed_through_nlp_text)
        declination_ner = declination_obj.decline_phrase_list(total_ner_list, preserve_case=True)

        # –°–∫–ª–æ–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        print(f'–°–∫–ª–æ–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑')
        declined_key_phrases: list[PatternKeyPhrases] = []
        for item in key_phrases:
            original_phrases = [phrase.key_phrase for phrase in item.found_key_phrases]
            declination_phrases = declination_obj.decline_phrase_list(original_phrases, preserve_case=False)
            declined_key_phrases.append(PatternKeyPhrases(
                item.pattern_config,
                declination_phrases
            ))

        print(f'–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞')
        return OutputPipelineData(
            declined_key_phrases,
            declination_ner
        )

    @staticmethod
    def print_results(final_results: OutputPipelineData):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        # –í—ã–≤–æ–¥ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—Ä–∞–∑
        print("\n=== –ö–õ–Æ–ß–ï–í–´–ï –§–†–ê–ó–´ ===")
        for idx, item in enumerate(final_results.key_phrases_obj):
            config = item.pattern_config
            current_phrases = item.key_phrases
            print(f"\nüîπ{config}")
            for j, phrase in enumerate(current_phrases, 1):
                print(f"  {j:2d}. {phrase}")

        # –í—ã–≤–æ–¥ NER-—Å—É—â–Ω–æ—Å—Ç–µ–π
        print("\n=== NER-–°–£–©–ù–û–°–¢–ò ===")
        for j, phrase in enumerate(final_results.ner_phrases, 1):
            print(f"  {j:2d}. {phrase}")

